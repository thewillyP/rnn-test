{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1000, 500, 2), Y shape: (1000, 500, 2)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "add got incompatible shapes for broadcasting: (20, 64), (128, 64).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 132\u001b[39m\n\u001b[32m    129\u001b[39m params = init_rnn_params(hidden_size, input_size, output_size, new_prng)\n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m trained_params = \u001b[43mtrain_rnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_prng\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 98\u001b[39m, in \u001b[36mtrain_rnn\u001b[39m\u001b[34m(params, X, Y, hidden_size, truncate_steps, learning_rate, num_epochs, batch_size, rng_key)\u001b[39m\n\u001b[32m     96\u001b[39m     chunk_X = batch_X[:, chunk, :, :]  \u001b[38;5;66;03m# Shape: (batch_size, truncate_steps, input_size)\u001b[39;00m\n\u001b[32m     97\u001b[39m     chunk_Y = batch_Y[:, chunk, :, :]  \u001b[38;5;66;03m# Shape: (batch_size, truncate_steps, output_size)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     params, opt_state, loss, h_init = \u001b[43mupdate_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_Y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m     batch_loss += loss\n\u001b[32m    101\u001b[39m total_loss += batch_loss / num_chunks\n",
      "    \u001b[31m[... skipping hidden 14 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 63\u001b[39m, in \u001b[36mupdate_step\u001b[39m\u001b[34m(params, opt_state, h_init, xs, ys)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;129m@jit\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupdate_step\u001b[39m(params, opt_state, h_init, xs, ys):\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     loss, grads = \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m     updates, opt_state = optimizer.update(grads, opt_state, params)\n\u001b[32m     65\u001b[39m     params = optax.apply_updates(params, updates)\n",
      "    \u001b[31m[... skipping hidden 16 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mcompute_loss\u001b[39m\u001b[34m(params, h_init, xs, ys)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_loss\u001b[39m(params, h_init, xs, ys):\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     _, logits = \u001b[43mrnn_scan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cross_entropy_loss(logits, ys)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mrnn_scan\u001b[39m\u001b[34m(params, h_init, xs)\u001b[39m\n\u001b[32m     46\u001b[39m     h_new, logits = rnn_step(params, h, x)\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m h_new, logits\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m h_final, logits = \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m h_final, logits\n",
      "    \u001b[31m[... skipping hidden 10 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36mrnn_scan.<locals>.step\u001b[39m\u001b[34m(carry, x)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(carry, x):\n\u001b[32m     45\u001b[39m     h = carry\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     h_new, logits = \u001b[43mrnn_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m h_new, logits\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mrnn_step\u001b[39m\u001b[34m(params, h, x)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrnn_step\u001b[39m(params, h, x):\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     h_new = jnp.tanh(\u001b[43mjnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mWxh\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mjnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mWhh\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m + params[\u001b[33m'\u001b[39m\u001b[33mbh\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     40\u001b[39m     logits = jnp.dot(h_new, params[\u001b[33m'\u001b[39m\u001b[33mWhy\u001b[39m\u001b[33m'\u001b[39m]) + params[\u001b[33m'\u001b[39m\u001b[33mby\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m h_new, logits\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.13/site-packages/jax/_src/numpy/array_methods.py:1060\u001b[39m, in \u001b[36m_forward_operator_to_aval.<locals>.op\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1059\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mop\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args):\n\u001b[32m-> \u001b[39m\u001b[32m1060\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.13/site-packages/jax/_src/numpy/array_methods.py:579\u001b[39m, in \u001b[36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    577\u001b[39m args = (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[32m    581\u001b[39m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.13/site-packages/jax/_src/numpy/ufunc_api.py:180\u001b[39m, in \u001b[36mufunc.__call__\u001b[39m\u001b[34m(self, out, where, *args)\u001b[39m\n\u001b[32m    178\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwhere argument of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    179\u001b[39m call = \u001b[38;5;28mself\u001b[39m.__static_props[\u001b[33m'\u001b[39m\u001b[33mcall\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_vectorized\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 14 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.13/site-packages/jax/_src/numpy/ufuncs.py:1215\u001b[39m, in \u001b[36madd\u001b[39m\u001b[34m(x, y)\u001b[39m\n\u001b[32m   1189\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Add two arrays element-wise.\u001b[39;00m\n\u001b[32m   1190\u001b[39m \n\u001b[32m   1191\u001b[39m \u001b[33;03mJAX implementation of :obj:`numpy.add`. This is a universal function,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1212\u001b[39m \u001b[33;03m  Array([10, 11, 12, 13], dtype=int32)\u001b[39;00m\n\u001b[32m   1213\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1214\u001b[39m x, y = promote_args(\u001b[33m\"\u001b[39m\u001b[33madd\u001b[39m\u001b[33m\"\u001b[39m, x, y)\n\u001b[32m-> \u001b[39m\u001b[32m1215\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlax\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m x.dtype != \u001b[38;5;28mbool\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m lax.bitwise_or(x, y)\n",
      "    \u001b[31m[... skipping hidden 10 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.13/site-packages/jax/_src/lax/lax.py:135\u001b[39m, in \u001b[36m_try_broadcast_shapes\u001b[39m\u001b[34m(name, *shapes)\u001b[39m\n\u001b[32m    133\u001b[39m       result_shape.append(non_1s[\u001b[32m0\u001b[39m])\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m got incompatible shapes for broadcasting: \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    136\u001b[39m                       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mtuple\u001b[39m,\u001b[38;5;250m \u001b[39mshapes)))\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(result_shape)\n",
      "\u001b[31mTypeError\u001b[39m: add got incompatible shapes for broadcasting: (20, 64), (128, 64)."
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, grad, jit\n",
    "import optax\n",
    "\n",
    "# Dataset generation\n",
    "def generate_add_task_dataset(num_examples, timesteps, t1, t2, tau_task, rng_key):\n",
    "    Xs, Ys = [], []\n",
    "    for _ in range(num_examples):\n",
    "        rng_key, subkey = random.split(rng_key)\n",
    "        X, Y = generate_single_sequence(timesteps, t1, t2, tau_task, subkey)\n",
    "        Xs.append(X)\n",
    "        Ys.append(Y)\n",
    "    return jnp.stack(Xs), jnp.stack(Ys)  # Shape: (num_examples, timesteps, 2)\n",
    "\n",
    "def generate_single_sequence(timesteps, t1, t2, tau_task, rng_key):\n",
    "    N = timesteps // tau_task\n",
    "    x = jax.random.bernoulli(rng_key, 0.5, (N,)).astype(jnp.float32)\n",
    "    y = 0.5 + 0.5 * jnp.roll(x, t1) - 0.25 * jnp.roll(x, t2)\n",
    "    X = jnp.asarray([x, 1 - x]).T\n",
    "    Y = jnp.asarray([y, 1 - y]).T\n",
    "    X = jnp.tile(X, tau_task).reshape((timesteps, 2))\n",
    "    Y = jnp.tile(Y, tau_task).reshape((timesteps, 2))\n",
    "    return X, Y\n",
    "\n",
    "# RNN model\n",
    "def init_rnn_params(hidden_size, input_size, output_size, rng_key):\n",
    "    k1, k2, k3, k4 = random.split(rng_key, 4)\n",
    "    return {\n",
    "        'Wxh': random.normal(k1, (input_size, hidden_size)) * 0.01,\n",
    "        'Whh': random.normal(k2, (hidden_size, hidden_size)) * 0.01,\n",
    "        'bh': jnp.zeros((hidden_size,)),\n",
    "        'Why': random.normal(k3, (hidden_size, output_size)) * 0.01,\n",
    "        'by': jnp.zeros((output_size,))\n",
    "    }\n",
    "\n",
    "def rnn_step(params, h, x):\n",
    "    # x: (batch_size, input_size), h: (batch_size, hidden_size)\n",
    "    h_new = jnp.tanh(jnp.dot(x, params['Wxh']) + jnp.dot(h, params['Whh']) + params['bh'])\n",
    "    logits = jnp.dot(h_new, params['Why']) + params['by']\n",
    "    return h_new, logits\n",
    "\n",
    "def rnn_scan(params, h_init, xs):\n",
    "    # xs: (batch_size, truncate_steps, input_size), h_init: (batch_size, hidden_size)\n",
    "    def step(carry, x):\n",
    "        h = carry\n",
    "        h_new, logits = rnn_step(params, h, x)\n",
    "        return h_new, logits\n",
    "    h_final, logits = jax.lax.scan(step, h_init, xs, dimension=1)  # Scan over truncate_steps\n",
    "    return h_final, logits  # logits: (batch_size, truncate_steps, output_size)\n",
    "\n",
    "# Loss function\n",
    "def cross_entropy_loss(logits, targets):\n",
    "    # logits, targets: (batch_size, truncate_steps, output_size)\n",
    "    probs = jax.nn.softmax(logits, axis=-1)\n",
    "    return -jnp.mean(jnp.sum(targets * jnp.log(probs + 1e-10), axis=-1))\n",
    "\n",
    "def compute_loss(params, h_init, xs, ys):\n",
    "    _, logits = rnn_scan(params, h_init, xs)\n",
    "    return cross_entropy_loss(logits, ys)\n",
    "\n",
    "# Training with TBPTT\n",
    "@jit\n",
    "def update_step(params, opt_state, h_init, xs, ys):\n",
    "    loss, grads = jax.value_and_grad(compute_loss)(params, h_init, xs, ys)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    h_final, _ = rnn_scan(params, h_init, xs)\n",
    "    return params, opt_state, loss, h_final\n",
    "\n",
    "def train_rnn(params, X, Y, hidden_size, truncate_steps, learning_rate, num_epochs, batch_size, rng_key):\n",
    "    optimizer = optax.sgd(learning_rate)  # SGD as requested\n",
    "    opt_state = optimizer.init(params)\n",
    "\n",
    "    num_examples = X.shape[0]  # 1000\n",
    "    timesteps = X.shape[1]  # 500\n",
    "    num_chunks = timesteps // truncate_steps  # 500 / 20 = 25\n",
    "\n",
    "    # Reshape X and Y into chunks\n",
    "    X_chunks = X.reshape(num_examples, num_chunks, truncate_steps, input_size)\n",
    "    Y_chunks = Y.reshape(num_examples, num_chunks, truncate_steps, output_size)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        rng_key, subkey = random.split(rng_key)\n",
    "        indices = random.permutation(subkey, num_examples)\n",
    "        total_loss = 0.0\n",
    "        num_batches = (num_examples + batch_size - 1) // batch_size\n",
    "\n",
    "        for i in range(0, num_examples, batch_size):\n",
    "            batch_indices = indices[i:min(i + batch_size, num_examples)]\n",
    "            batch_X = X_chunks[batch_indices]  # (batch_size, num_chunks, truncate_steps, input_size)\n",
    "            batch_Y = Y_chunks[batch_indices]  # (batch_size, num_chunks, truncate_steps, output_size)\n",
    "            h_init = jnp.zeros((batch_indices.shape[0], hidden_size))  # Dynamic batch size\n",
    "\n",
    "            batch_loss = 0.0\n",
    "            for chunk in range(num_chunks):\n",
    "                chunk_X = batch_X[:, chunk, :, :]  # (batch_size, truncate_steps, input_size)\n",
    "                chunk_Y = batch_Y[:, chunk, :, :]  # (batch_size, truncate_steps, output_size)\n",
    "                params, opt_state, loss, h_init = update_step(params, opt_state, h_init, chunk_X, chunk_Y)\n",
    "                batch_loss += loss\n",
    "\n",
    "            total_loss += batch_loss / num_chunks\n",
    "\n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    return params\n",
    "\n",
    "# Parameters\n",
    "t1, t2 = 3, 5\n",
    "tau_task = 1\n",
    "num_examples = 1000\n",
    "timesteps = 500\n",
    "input_size = 2\n",
    "output_size = 2\n",
    "truncate_steps = 20\n",
    "hidden_size = 64\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "# Generate dataset\n",
    "prng = random.PRNGKey(0)\n",
    "prng, new_prng = random.split(prng)\n",
    "X, Y = generate_add_task_dataset(num_examples, timesteps, t1, t2, tau_task, new_prng)\n",
    "print(f\"X shape: {X.shape}, Y shape: {Y.shape}\")  # (1000, 500, 2)\n",
    "\n",
    "# Initialize model\n",
    "prng, new_prng = random.split(prng)\n",
    "params = init_rnn_params(hidden_size, input_size, output_size, new_prng)\n",
    "\n",
    "# Train\n",
    "trained_params = train_rnn(params, X, Y, hidden_size, truncate_steps, learning_rate, num_epochs, batch_size, new_prng)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
