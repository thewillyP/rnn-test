{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Grid Analysis with Whisker Plots for Test Loss and Accuracy\n",
    "# This notebook processes machine learning run data from two groups (`mlr_search` and `fixed_search`) to generate a grid plot of inner learning rate by outer learning rate. Each grid cell contains four whisker plots: test loss and accuracy for the main group, and test loss and accuracy for the fixed group using the best inner learning rate based on lowest average validation loss.\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd())\n",
    "sys.path.append(os.path.join(os.getcwd(), '../src'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import multiprocessing\n",
    "from recurrent.parameters import AllLogs\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "jax.config.update(\"jax_platform_name\", \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "        logging.FileHandler('grid_analysis_whisker.log')\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set multiprocessing start method to 'spawn' to avoid JAX fork issues\n",
    "multiprocessing.set_start_method(\"spawn\", force=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Configuration\n",
    "download_dir = \"/scratch/downloaded_artifacts\"\n",
    "results_dir = \"/scratch/results\"\n",
    "group_name = \"mlr_search-5_abcdef123456\"\n",
    "fixed_group_name = \"fixed_search-5_ghijk789012\"\n",
    "max_process_workers = 10\n",
    "success_threshold = 0.95\n",
    "y_limits_loss = (0.4, 0.8)  # Y-axis limits for test loss\n",
    "y_limits_acc = (0.0, 1.0)  # Y-axis limits for accuracy\n",
    "lr_failure_threshold = 1e-4\n",
    "config_keys = [\"inner_optimizer\", \"inner_learner\"]  # Config keys for grouping\n",
    "\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_folder_name(name):\n",
    "    return re.sub(r'[^\\w\\-]', '_', str(name))\n",
    "\n",
    "# %%\n",
    "def process_run(run_result):\n",
    "    run_id = run_result[\"run_id\"]\n",
    "    artifact_dir = run_result[\"artifact_dir\"]\n",
    "    config = run_result[\"config\"]\n",
    "    summary = run_result.get(\"summary\", {})\n",
    "    \n",
    "    if run_result[\"status\"] != \"success\" or not artifact_dir or not config:\n",
    "        logger.warning(f\"Skipping run {run_id}: download failed or no config\")\n",
    "        return {\n",
    "            \"run_id\": run_id,\n",
    "            \"status\": \"skipped\",\n",
    "            \"data\": None\n",
    "        }\n",
    "    \n",
    "    log_file = os.path.join(artifact_dir, \"logs.pkl\")\n",
    "    if not os.path.exists(log_file):\n",
    "        logger.error(f\"Logs file not found for run {run_id}\")\n",
    "        return {\n",
    "            \"run_id\": run_id,\n",
    "            \"status\": \"no_log_file\",\n",
    "            \"data\": None\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        with open(log_file, \"rb\") as f:\n",
    "            logs = pickle.load(f)\n",
    "        \n",
    "        if not isinstance(logs, AllLogs):\n",
    "            logger.error(f\"Logs for run {run_id} is not an AllLogs instance\")\n",
    "            return {\n",
    "                \"run_id\": run_id,\n",
    "                \"status\": \"invalid_logs\",\n",
    "                \"data\": None\n",
    "            }\n",
    "        \n",
    "        is_success = not np.any(logs.inner_learning_rate <= lr_failure_threshold)\n",
    "        final_test_loss = float(logs.test_loss[-1]) if logs.test_loss is not None else None\n",
    "        final_validation_loss = float(logs.validation_loss[-1]) if logs.validation_loss is not None else None\n",
    "        test_statistic = float(summary.get(\"test_statistic\")) if summary.get(\"test_statistic\") is not None else None\n",
    "        \n",
    "        logger.info(f\"Processed run {run_id}: success={is_success}, final_test_loss={final_test_loss}, accuracy={test_statistic}\")\n",
    "        \n",
    "        return {\n",
    "            \"run_id\": run_id,\n",
    "            \"status\": \"success\",\n",
    "            \"data\": {\n",
    "                \"config\": config,\n",
    "                \"is_success\": is_success,\n",
    "                \"final_test_loss\": final_test_loss,\n",
    "                \"final_validation_loss\": final_validation_loss,\n",
    "                \"test_statistic\": test_statistic,\n",
    "                \"inner_optimizer\": config.get(\"inner_optimizer\", \"unknown\"),\n",
    "                \"inner_learner\": config.get(\"inner_learner\", \"unknown\"),\n",
    "                \"inner_learning_rate\": config.get(\"inner_learning_rate\"),\n",
    "                \"outer_learning_rate\": config.get(\"outer_learning_rate\")\n",
    "            }\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing logs for run {run_id}: {str(e)}\")\n",
    "        return {\n",
    "            \"run_id\": run_id,\n",
    "            \"status\": f\"error: {str(e)}\",\n",
    "            \"data\": None\n",
    "        }\n",
    "\n",
    "# %%\n",
    "def process_fixed_group_data(fixed_download_results):\n",
    "    \"\"\"Process fixed_search data to get test loss and accuracy for best inner learning rate per outer learning rate.\"\"\"\n",
    "    with ThreadPoolExecutor(max_workers=max_process_workers) as executor:\n",
    "        fixed_process_results = list(executor.map(process_run, fixed_download_results))\n",
    "    \n",
    "    # Include all runs with status=success and valid data\n",
    "    fixed_runs_data = [\n",
    "        result[\"data\"] for result in fixed_process_results\n",
    "        if result[\"status\"] == \"success\" and result[\"data\"]\n",
    "    ]\n",
    "    \n",
    "    # Organize by outer learning rate\n",
    "    outer_lr_runs = defaultdict(list)\n",
    "    for run in fixed_runs_data:\n",
    "        outer_lr = run[\"outer_learning_rate\"]\n",
    "        outer_lr_runs[outer_lr].append(run)\n",
    "    \n",
    "    # For each outer learning rate, find the inner learning rate with lowest average validation loss\n",
    "    outer_lr_stats = {}\n",
    "    for outer_lr in outer_lr_runs:\n",
    "        runs = outer_lr_runs[outer_lr]\n",
    "        # Group by inner learning rate\n",
    "        lr_val_losses = defaultdict(list)\n",
    "        for run in runs:\n",
    "            if run[\"is_success\"] and run[\"final_validation_loss\"] is not None:\n",
    "                lr_val_losses[run[\"inner_learning_rate\"]].append(run[\"final_validation_loss\"])\n",
    "        \n",
    "        # Find the inner learning rate with lowest average validation loss\n",
    "        best_lr = None\n",
    "        lowest_avg_val_loss = float('inf')\n",
    "        for lr, val_losses in lr_val_losses.items():\n",
    "            avg_val_loss = np.mean(val_losses)\n",
    "            if avg_val_loss < lowest_avg_val_loss:\n",
    "                lowest_avg_val_loss = avg_val_loss\n",
    "                best_lr = lr\n",
    "        \n",
    "        if best_lr is not None:\n",
    "            # Collect test losses and accuracies for runs with the best inner learning rate\n",
    "            test_losses = [\n",
    "                run[\"final_test_loss\"] for run in runs\n",
    "                if run[\"inner_learning_rate\"] == best_lr and run[\"is_success\"] and run[\"final_test_loss\"] is not None\n",
    "            ]\n",
    "            accuracies = [\n",
    "                run[\"test_statistic\"] for run in runs\n",
    "                if run[\"inner_learning_rate\"] == best_lr and run[\"is_success\"] and run[\"test_statistic\"] is not None\n",
    "            ]\n",
    "            if test_losses or accuracies:\n",
    "                outer_lr_stats[outer_lr] = {\n",
    "                    \"test_losses\": test_losses,\n",
    "                    \"accuracies\": accuracies,\n",
    "                    \"best_lr\": best_lr\n",
    "                }\n",
    "    \n",
    "    return outer_lr_stats\n",
    "\n",
    "# %%\n",
    "def save_best_lr_figure(fixed_outer_lr_stats, group_name, config_combination):\n",
    "    \"\"\"Save a figure listing the best inner learning rates for each outer learning rate.\"\"\"\n",
    "    fig = plt.figure(figsize=(4, len(fixed_outer_lr_stats) * 0.5 + 1))\n",
    "    lr_text = \"Best Inner Learning Rates (Fixed Search):\\n\"\n",
    "    for outer_lr in sorted(fixed_outer_lr_stats.keys()):\n",
    "        best_lr = fixed_outer_lr_stats[outer_lr].get(\"best_lr\")\n",
    "        if best_lr is not None:\n",
    "            lr_text += f\"Outer LR={outer_lr:.1e}: {best_lr:.1e}\\n\"\n",
    "    \n",
    "    plt.text(0.1, 0.9, lr_text, fontsize=10, verticalalignment='top')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Create subfolder based on config combination\n",
    "    subfolder = \"_\".join(\n",
    "        f\"{sanitize_folder_name(key)}_{sanitize_folder_name(value)}\"\n",
    "        for key, value in config_combination\n",
    "    )\n",
    "    group_results_dir = os.path.join(results_dir, group_name, subfolder)\n",
    "    os.makedirs(group_results_dir, exist_ok=True)\n",
    "    output_file = os.path.join(group_results_dir, 'best_learning_rates.png')\n",
    "    plt.savefig(output_file, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    logger.info(f\"Saved best learning rates figure for {subfolder} to {output_file}\")\n",
    "\n",
    "# %%\n",
    "def create_grid_plot(runs_data, fixed_outer_lr_stats, config_combination, group_name):\n",
    "    # Filter runs by the specified configuration combination\n",
    "    filtered_runs = [\n",
    "        run for run in runs_data\n",
    "        if all(run[\"config\"].get(key, \"unknown\") == value for key, value in config_combination)\n",
    "    ]\n",
    "    \n",
    "    if not filtered_runs:\n",
    "        logger.warning(f\"No runs found for config combination: {config_combination}\")\n",
    "        return\n",
    "    \n",
    "    # Organize data by outer and inner learning rates\n",
    "    lr_grid = defaultdict(list)\n",
    "    for run in filtered_runs:\n",
    "        outer_lr = run[\"config\"].get(\"outer_learning_rate\")\n",
    "        inner_lr = run[\"config\"].get(\"inner_learning_rate\")\n",
    "        lr_grid[(outer_lr, inner_lr)].append(run)\n",
    "    \n",
    "    # Collect outer and inner learning rates\n",
    "    outer_lrs = sorted(set(run[\"config\"].get(\"outer_learning_rate\") for run in filtered_runs))\n",
    "    inner_lrs = sorted(set(run[\"config\"].get(\"inner_learning_rate\") for run in filtered_runs))\n",
    "    \n",
    "    if not outer_lrs or not inner_lrs:\n",
    "        logger.warning(f\"No valid learning rates for config combination: {config_combination}\")\n",
    "        return\n",
    "    \n",
    "    # Create grid plot\n",
    "    fig, axes = plt.subplots(len(inner_lrs), len(outer_lrs), figsize=(len(outer_lrs) * 4, len(inner_lrs) * 4), squeeze=False)\n",
    "    \n",
    "    for i, inner_lr in enumerate(inner_lrs):\n",
    "        for j, outer_lr in enumerate(outer_lrs):\n",
    "            ax = axes[i, j]\n",
    "            runs = lr_grid.get((outer_lr, inner_lr), [])\n",
    "            \n",
    "            if not runs:\n",
    "                ax.axis('off')\n",
    "                continue\n",
    "            \n",
    "            # Collect test losses and accuracies for main group\n",
    "            test_losses = [\n",
    "                run[\"final_test_loss\"] for run in runs\n",
    "                if run[\"is_success\"] and run[\"final_test_loss\"] is not None\n",
    "            ]\n",
    "            accuracies = [\n",
    "                run[\"test_statistic\"] for run in runs\n",
    "                if run[\"is_success\"] and run[\"test_statistic\"] is not None\n",
    "            ]\n",
    "            \n",
    "            # Get test losses and accuracies for fixed group\n",
    "            fixed_test_losses = fixed_outer_lr_stats.get(outer_lr, {}).get(\"test_losses\", [])\n",
    "            fixed_accuracies = fixed_outer_lr_stats.get(outer_lr, {}).get(\"accuracies\", [])\n",
    "            \n",
    "            # Prepare data for box plots\n",
    "            data = []\n",
    "            labels = []\n",
    "            colors = []\n",
    "            if test_losses:\n",
    "                data.append(test_losses)\n",
    "                labels.append('MLR Loss')\n",
    "                colors.append('blue')\n",
    "            if accuracies:\n",
    "                data.append(accuracies)\n",
    "                labels.append('MLR Acc')\n",
    "                colors.append('green')\n",
    "            if fixed_test_losses:\n",
    "                data.append(fixed_test_losses)\n",
    "                labels.append('Fixed Loss')\n",
    "                colors.append('red')\n",
    "            if fixed_accuracies:\n",
    "                data.append(fixed_accuracies)\n",
    "                labels.append('Fixed Acc')\n",
    "                colors.append('purple')\n",
    "            \n",
    "            if data:\n",
    "                # Create box plots with asymmetric whiskers\n",
    "                box = ax.boxplot(\n",
    "                    data,\n",
    "                    labels=labels,\n",
    "                    patch_artist=True,\n",
    "                    showmeans=True,\n",
    "                    whis=[5, 95],  # Show 5th and 95th percentiles for asymmetric whiskers\n",
    "                    widths=0.2 if len(data) == 4 else (0.25 if len(data) == 3 else (0.33 if len(data) == 2 else 0.5))\n",
    "                )\n",
    "                \n",
    "                # Customize box plot colors\n",
    "                for patch, color in zip(box['boxes'], colors):\n",
    "                    patch.set_facecolor(color)\n",
    "                    patch.set_alpha(0.6)\n",
    "                \n",
    "                # Set y-limits based on data type (loss or accuracy)\n",
    "                y_min = min(y_limits_loss[0], y_limits_acc[0])\n",
    "                y_max = max(y_limits_loss[1], y_limits_acc[1])\n",
    "                ax.set_ylim(y_min, y_max)\n",
    "                \n",
    "                # Add secondary y-axis for accuracy\n",
    "                ax2 = ax.twinx()\n",
    "                ax2.set_ylim(y_limits_acc)\n",
    "                ax2.set_ylabel('Accuracy', color='green')\n",
    "                ax2.tick_params(axis='y', labelcolor='green')\n",
    "                \n",
    "                # Set primary y-axis for loss\n",
    "                ax.set_ylim(y_limits_loss)\n",
    "                ax.set_ylabel('Test Loss', color='blue')\n",
    "                ax.tick_params(axis='y', labelcolor='blue')\n",
    "                \n",
    "                ax.grid(True, linestyle='--', alpha=0.7)\n",
    "                \n",
    "                # Calculate success fraction for main group\n",
    "                total_runs = len(runs)\n",
    "                successful_runs = len([run for run in runs if run[\"is_success\"]])\n",
    "                ax.set_title(f'Success: {successful_runs}/{total_runs}', fontsize=8)\n",
    "            else:\n",
    "                ax.axis('off')\n",
    "    \n",
    "    # Add hyperaxis labels for outer and inner learning rates\n",
    "    for j, outer_lr in enumerate(outer_lrs):\n",
    "        fig.text(\n",
    "            (j + 0.5) / len(outer_lrs), 1.01,\n",
    "            f'Outer LR: {outer_lr:.1e}',\n",
    "            ha='center', va='bottom', fontsize=10, transform=fig.transFigure\n",
    "        )\n",
    "    \n",
    "    for i, inner_lr in enumerate(inner_lrs):\n",
    "        fig.text(\n",
    "            -0.01, (len(inner_lrs) - i - 0.5) / len(inner_lrs),\n",
    "            f'Inner LR: {inner_lr:.1e}',\n",
    "            ha='right', va='center', fontsize=10, transform=fig.transFigure\n",
    "        )\n",
    "    \n",
    "    # Create title based on config combination\n",
    "    config_title = \", \".join(f\"{key}: {value}\" for key, value in config_combination)\n",
    "    plt.suptitle(f'Test Loss and Accuracy by Learning Rates\\n(Group: {group_name}, {config_title})', fontsize=14)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    \n",
    "    # Save plot\n",
    "    subfolder = \"_\".join(\n",
    "        f\"{sanitize_folder_name(key)}_{sanitize_folder_name(value)}\"\n",
    "        for key, value in config_combination\n",
    "    )\n",
    "    group_results_dir = os.path.join(results_dir, group_name, subfolder)\n",
    "    os.makedirs(group_results_dir, exist_ok=True)\n",
    "    output_file = os.path.join(group_results_dir, 'test_loss_accuracy_whisker_grid.png')\n",
    "    plt.savefig(output_filecents, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    logger.info(f\"Saved whisker grid plot for {subfolder} to {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Load and process main group data\n",
    "download_results_file = os.path.join(download_dir, f'download_results_{group_name}.pkl')\n",
    "if not os.path.exists(download_results_file):\n",
    "    logger.error(f\"Download results file not found at {download_results_file}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "with open(download_results_file, 'rb') as f:\n",
    "    download_results = pickle.load(f)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_process_workers) as executor:\n",
    "    process_results = list(executor.map(process_run, download_results))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_runs_data = [\n",
    "    result[\"data\"] for result in process_results\n",
    "    if result[\"status\"] == \"success\" and result[\"data\"]\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Load and process fixed_search data\n",
    "fixed_download_results_file = os.path.join(download_dir, f'download_results_{fixed_group_name}.pkl')\n",
    "if not os.path.exists(fixed_download_results_file):\n",
    "    logger.error(f\"Fixed search results file not found at {fixed_download_results_file}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "with open(fixed_download_results_file, 'rb') as f:\n",
    "    fixed_download_results = pickle.load(f)\n",
    "\n",
    "fixed_outer_lr_stats = process_fixed_group_data(fixed_download_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Generate plots for each config combination\n",
    "config_combinations = set()\n",
    "for run in all_runs_data:\n",
    "    combination = tuple((key, run[\"config\"].get(key, \"unknown\")) for key in config_keys)\n",
    "    config_combinations.add(combination)\n",
    "\n",
    "for config_combination in sorted(config_combinations):\n",
    "    logger.info(f\"Generating whisker plot for config combination: {config_combination}\")\n",
    "    create_grid_plot(all_runs_data, fixed_outer_lr_stats, config_combination, group_name)\n",
    "    save_best_lr_figure(fixed_outer_lr_stats, group_name, config_combination)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
