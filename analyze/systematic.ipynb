{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd())\n",
    "sys.path.append(os.getcwd() + '/../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import wandb\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import multiprocessing\n",
    "import logging\n",
    "import sys\n",
    "from recurrent.parameters import AllLogs\n",
    "\n",
    "import jax \n",
    "import jax.numpy as jnp\n",
    "jax.config.update(\"jax_platform_name\", \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "        logging.FileHandler('processing.log')\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set multiprocessing start method to 'spawn' to avoid JAX fork issues\n",
    "multiprocessing.set_start_method(\"spawn\", force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "download_dir = \"downloaded_artifacts\"\n",
    "results_dir = \"results\"\n",
    "entity = \"wlp9800-new-york-university\"\n",
    "project_name = \"oho_exps\"\n",
    "group_name = \"time_test_oho_d1efc05e0903463ca4e95a52714389a0\"\n",
    "max_download_workers = 20\n",
    "max_process_workers = 10  # Reduced for ThreadPoolExecutor stability\n",
    "\n",
    "# Ensure download directory exists\n",
    "os.makedirs(download_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download a single run's artifact\n",
    "def download_artifact(run_data):\n",
    "    run_id = run_data[\"id\"]\n",
    "    config = run_data[\"config\"]\n",
    "    try:\n",
    "        api = wandb.Api()\n",
    "        artifact = api.artifact(f'{entity}/{project_name}/logs_{run_id}:v0')\n",
    "        artifact_dir = os.path.join(download_dir, artifact.name)\n",
    "        artifact.download(root=artifact_dir)\n",
    "        logger.info(f\"Downloaded {artifact.name} to {artifact_dir}\")\n",
    "        return {\n",
    "            \"run_id\": run_id,\n",
    "            \"artifact_dir\": artifact_dir,\n",
    "            \"config\": config,\n",
    "            \"status\": \"success\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error downloading artifact for run {run_id}: {str(e)}\")\n",
    "        return {\n",
    "            \"run_id\": run_id,\n",
    "            \"artifact_dir\": None,\n",
    "            \"config\": config,\n",
    "            \"status\": f\"error: {str(e)}\"\n",
    "        }\n",
    "\n",
    "# Function to process a single run's logs\n",
    "def process_run(run_result):\n",
    "    run_id = run_result[\"run_id\"]\n",
    "    artifact_dir = run_result[\"artifact_dir\"]\n",
    "    config = run_result[\"config\"]\n",
    "    \n",
    "    if run_result[\"status\"] != \"success\" or not artifact_dir:\n",
    "        logger.warning(f\"Skipping run {run_id}: download failed\")\n",
    "        return {\n",
    "            \"run_id\": run_id,\n",
    "            \"status\": \"skipped\",\n",
    "            \"data\": None\n",
    "        }\n",
    "    \n",
    "    log_file = os.path.join(artifact_dir, \"logs.pkl\")\n",
    "    if not os.path.exists(log_file):\n",
    "        logger.error(f\"Logs file not found for run {run_id}\")\n",
    "        return {\n",
    "            \"run_id\": run_id,\n",
    "            \"status\": \"no_log_file\",\n",
    "            \"data\": None\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        with open(log_file, \"rb\") as f:\n",
    "            logs = pickle.load(f)\n",
    "        \n",
    "        if not isinstance(logs, AllLogs):\n",
    "            logger.error(f\"Logs for run {run_id} is not an AllLogs instance\")\n",
    "            return {\n",
    "                \"run_id\": run_id,\n",
    "                \"status\": \"invalid_logs\",\n",
    "                \"data\": None\n",
    "            }\n",
    "        \n",
    "        ts = tuple(config.get(\"ts\", ()))\n",
    "        outer_lr = config.get(\"outer_learning_rate\")\n",
    "        inner_lr = config.get(\"inner_learning_rate\")\n",
    "        \n",
    "        if outer_lr is None or inner_lr is None or not ts:\n",
    "            logger.error(f\"Missing config values for run {run_id}\")\n",
    "            return {\n",
    "                \"run_id\": run_id,\n",
    "                \"status\": \"missing_config\",\n",
    "                \"data\": None\n",
    "            }\n",
    "        \n",
    "        is_success = not jnp.any(logs.hyperparameters == 1e-4)\n",
    "        logger.info(f\"Processed run {run_id}: success={is_success}\")\n",
    "        \n",
    "        return {\n",
    "            \"run_id\": run_id,\n",
    "            \"status\": \"success\",\n",
    "            \"data\": {\n",
    "                \"ts\": ts,\n",
    "                \"outer_lr\": outer_lr,\n",
    "                \"inner_lr\": inner_lr,\n",
    "                \"is_success\": is_success\n",
    "            }\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing logs for run {run_id}: {str(e)}\")\n",
    "        return {\n",
    "            \"run_id\": run_id,\n",
    "            \"status\": f\"error: {str(e)}\",\n",
    "            \"data\": None\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heatmap(ts, runs_data, group_idx, group_name):\n",
    "    outer_lrs = sorted(set(run[\"outer_lr\"] for run in runs_data))\n",
    "    inner_lrs = sorted(set(run[\"inner_lr\"] for run in runs_data))\n",
    "    \n",
    "    if not outer_lrs or not inner_lrs:\n",
    "        logger.warning(f\"No valid learning rates for ts group {ts} in {group_name}\")\n",
    "        return\n",
    "    \n",
    "    grid = np.zeros((len(inner_lrs), len(outer_lrs)))\n",
    "    success_counts = defaultdict(int)\n",
    "    total_counts = defaultdict(int)\n",
    "    \n",
    "    for run in runs_data:\n",
    "        outer_idx = outer_lrs.index(run[\"outer_lr\"])\n",
    "        inner_idx = inner_lrs.index(run[\"inner_lr\"])\n",
    "        key = (inner_idx, outer_idx)\n",
    "        total_counts[key] += 1\n",
    "        if run[\"is_success\"]:\n",
    "            success_counts[key] += 1\n",
    "    \n",
    "    for (inner_idx, outer_idx), total in total_counts.items():\n",
    "        successes = success_counts.get((inner_idx, outer_idx), 0)\n",
    "        grid[inner_idx, outer_idx] = successes / total if total > 0 else 0\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    im = plt.imshow(grid, origin='lower', cmap='viridis', interpolation='nearest')\n",
    "    plt.colorbar(im, label='Fraction of Successful Runs')\n",
    "    \n",
    "    for i in range(len(inner_lrs)):\n",
    "        for j in range(len(outer_lrs)):\n",
    "            fraction = grid[i, j]\n",
    "            text = f\"{fraction:.2f}\" if total_counts.get((i, j), 0) > 0 else \"N/A\"\n",
    "            plt.text(j, i, text, ha='center', va='center', color='white' if fraction < 0.5 else 'black')\n",
    "    \n",
    "    plt.xticks(np.arange(len(outer_lrs)), [f\"{lr:.1e}\" for lr in outer_lrs], rotation=45)\n",
    "    plt.yticks(np.arange(len(inner_lrs)), [f\"{lr:.1e}\" for lr in inner_lrs])\n",
    "    plt.xlabel('Outer Learning Rate')\n",
    "    plt.ylabel('Inner Learning Rate')\n",
    "    plt.title(f'Success Fraction Heatmap for ts={ts} in {group_name}')\n",
    "    \n",
    "    # Save to group-specific results directory\n",
    "    group_results_dir = os.path.join(results_dir, group_name)\n",
    "    os.makedirs(group_results_dir, exist_ok=True)\n",
    "    output_file = os.path.join(group_results_dir, f'heatmap_ts_group_{group_idx}.png')\n",
    "    plt.savefig(output_file, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    logger.info(f\"Saved heatmap for ts={ts} in {group_name} to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-18 23:07:05,448 - INFO - Found 10 runs to process\n"
     ]
    }
   ],
   "source": [
    "api = wandb.Api()\n",
    "runs = api.runs(\n",
    "    path=f\"{entity}/{project_name}\",\n",
    "    filters={\"group\": group_name}\n",
    ")\n",
    "\n",
    "# Prepare run data\n",
    "run_data = [{\"id\": run.id, \"config\": run.config} for run in runs]\n",
    "logger.info(f\"Found {len(run_data)} runs to process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-18 23:07:06,036 - INFO - Downloaded logs_gicdoysn:v0 to downloaded_artifacts/logs_gicdoysn:v0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-18 23:07:06,041 - INFO - Downloaded logs_y6klzrn2:v0 to downloaded_artifacts/logs_y6klzrn2:v0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-18 23:07:06,050 - INFO - Downloaded logs_mgwak6ta:v0 to downloaded_artifacts/logs_mgwak6ta:v0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-18 23:07:06,057 - INFO - Downloaded logs_83hjf1k8:v0 to downloaded_artifacts/logs_83hjf1k8:v0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-18 23:07:06,064 - INFO - Downloaded logs_wgeu3744:v0 to downloaded_artifacts/logs_wgeu3744:v0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-18 23:07:06,065 - INFO - Downloaded logs_drme1azv:v0 to downloaded_artifacts/logs_drme1azv:v0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-18 23:07:06,078 - INFO - Downloaded logs_lky8uxxr:v0 to downloaded_artifacts/logs_lky8uxxr:v0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-18 23:07:06,082 - INFO - Downloaded logs_270lhq5v:v0 to downloaded_artifacts/logs_270lhq5v:v0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-18 23:07:06,083 - INFO - Downloaded logs_9f568pgc:v0 to downloaded_artifacts/logs_9f568pgc:v0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-18 23:07:06,098 - INFO - Downloaded logs_vuae6fly:v0 to downloaded_artifacts/logs_vuae6fly:v0\n"
     ]
    }
   ],
   "source": [
    "with ThreadPoolExecutor(max_workers=max_download_workers) as executor:\n",
    "    download_results = list(executor.map(download_artifact, run_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-18 23:07:06,297 - INFO - Processed run 9f568pgc: success=False\n",
      "2025-04-18 23:07:06,301 - INFO - Processed run lky8uxxr: success=False\n",
      "2025-04-18 23:07:06,302 - INFO - Processed run vuae6fly: success=False\n",
      "2025-04-18 23:07:06,303 - INFO - Processed run wgeu3744: success=True\n",
      "2025-04-18 23:07:06,303 - INFO - Processed run 270lhq5v: success=False\n",
      "2025-04-18 23:07:06,303 - INFO - Processed run mgwak6ta: success=True\n",
      "2025-04-18 23:07:06,306 - INFO - Processed run drme1azv: success=True\n",
      "2025-04-18 23:07:06,306 - INFO - Processed run y6klzrn2: success=True\n",
      "2025-04-18 23:07:06,308 - INFO - Processed run gicdoysn: success=True\n",
      "2025-04-18 23:07:06,310 - INFO - Processed run 83hjf1k8: success=True\n"
     ]
    }
   ],
   "source": [
    "with ThreadPoolExecutor(max_workers=max_process_workers) as executor:\n",
    "    process_results = list(executor.map(process_run, download_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group successful runs by ts\n",
    "run_groups = defaultdict(list)\n",
    "for result in process_results:\n",
    "    if result[\"status\"] == \"success\" and result[\"data\"]:\n",
    "        run_groups[result[\"data\"][\"ts\"]].append(result[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-18 23:07:06,328 - INFO - Generating heatmap for ts group (14, 16) in time_test_oho_d1efc05e0903463ca4e95a52714389a0\n",
      "2025-04-18 23:07:06,564 - INFO - Saved heatmap for ts=(14, 16) in time_test_oho_d1efc05e0903463ca4e95a52714389a0 to results/time_test_oho_d1efc05e0903463ca4e95a52714389a0/heatmap_ts_group_0.png\n"
     ]
    }
   ],
   "source": [
    "for idx, (ts, runs_data) in enumerate(run_groups.items()):\n",
    "    logger.info(f\"Generating heatmap for ts group {ts} in {group_name}\")\n",
    "    create_heatmap(ts, runs_data, idx, group_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
