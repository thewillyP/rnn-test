{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd())\n",
    "sys.path.append(os.getcwd() + '/../src')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import multiprocessing\n",
    "from recurrent.parameters import AllLogs\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "jax.config.update(\"jax_platform_name\", \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "        logging.FileHandler('processing.log')\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set multiprocessing start method to 'spawn' to avoid JAX fork issues\n",
    "multiprocessing.set_start_method(\"spawn\", force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "download_dir = \"/scratch/downloaded_artifacts\"\n",
    "results_dir = \"/scratch/results\"\n",
    "group_name = \"mlr_search-1_aa9c06652fb34624bebe972b1fe7292f\"\n",
    "max_process_workers = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure results directory exists\n",
    "os.makedirs(results_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sanitize folder names\n",
    "def sanitize_folder_name(name):\n",
    "    return re.sub(r'[^\\w\\-]', '_', str(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single run's logs\n",
    "def process_run(run_result):\n",
    "    run_id = run_result[\"run_id\"]\n",
    "    artifact_dir = run_result[\"artifact_dir\"]\n",
    "    config = run_result[\"config\"]\n",
    "    \n",
    "    if run_result[\"status\"] != \"success\" or not artifact_dir or not config:\n",
    "        logger.warning(f\"Skipping run {run_id}: download failed or no config\")\n",
    "        return {\n",
    "            \"run_id\": run_id,\n",
    "            \"status\": \"skipped\",\n",
    "            \"data\": None\n",
    "        }\n",
    "    \n",
    "    log_file = os.path.join(artifact_dir, \"logs.pkl\")\n",
    "    if not os.path.exists(log_file):\n",
    "        logger.error(f\"Logs file not found for run {run_id}\")\n",
    "        return {\n",
    "            \"run_id\": run_id,\n",
    "            \"status\": \"no_log_file\",\n",
    "            \"data\": None\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        with open(log_file, \"rb\") as f:\n",
    "            logs = pickle.load(f)\n",
    "        \n",
    "        if not isinstance(logs, AllLogs):\n",
    "            logger.error(f\"Logs for run {run_id} is not an AllLogs instance\")\n",
    "            return {\n",
    "                \"run_id\": run_id,\n",
    "                \"status\": \"invalid_logs\",\n",
    "                \"data\": None\n",
    "            }\n",
    "        \n",
    "        is_success = not np.any(logs.hyperparameters == 1e-4)\n",
    "        logger.info(f\"Processed run {run_id}: success={is_success}\")\n",
    "        \n",
    "        return {\n",
    "            \"run_id\": run_id,\n",
    "            \"status\": \"success\",\n",
    "            \"data\": {\n",
    "                \"config\": config,\n",
    "                \"is_success\": is_success\n",
    "            }\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing logs for run {run_id}: {str(e)}\")\n",
    "        return {\n",
    "            \"run_id\": run_id,\n",
    "            \"status\": f\"error: {str(e)}\",\n",
    "            \"data\": None\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_results_file = os.path.join(download_dir, f'download_results_{group_name}.pkl')\n",
    "if not os.path.exists(download_results_file):\n",
    "    logger.error(f\"Download results file not found at {download_results_file}\")\n",
    "\n",
    "with open(download_results_file, 'rb') as f:\n",
    "    download_results = pickle.load(f)\n",
    "\n",
    "# Process downloaded artifacts\n",
    "with ThreadPoolExecutor(max_workers=max_process_workers) as executor:\n",
    "    process_results = list(executor.map(process_run, download_results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create heatmap\n",
    "def create_heatmap(ts, runs_data, group_idx, group_name, subfolder=None, inner_optimizer=None):\n",
    "    if inner_optimizer:\n",
    "        runs_data = [run for run in runs_data if inner_optimizer in (run[\"config\"].get(\"inner_optimizer\", [\"unknown\"]) if isinstance(run[\"config\"].get(\"inner_optimizer\"), list) else [run[\"config\"].get(\"inner_optimizer\", \"unknown\")])]\n",
    "    runs_data = [run for run in runs_data if tuple(run[\"config\"].get(\"ts\", ())) == ts]\n",
    "    outer_lrs = sorted(set(run[\"config\"].get(\"outer_learning_rate\") for run in runs_data))\n",
    "    inner_lrs = sorted(set(run[\"config\"].get(\"inner_learning_rate\") for run in runs_data))\n",
    "    \n",
    "    if not outer_lrs or not inner_lrs:\n",
    "        logger.warning(f\"No valid learning rates for ts group {ts} in {group_name}{f'/{subfolder}' if subfolder else ''}{f' (inner_optimizer={inner_optimizer})' if inner_optimizer else ''}\")\n",
    "        return\n",
    "    \n",
    "    grid = np.zeros((len(inner_lrs), len(outer_lrs)))\n",
    "    success_counts = defaultdict(int)\n",
    "    total_counts = defaultdict(int)\n",
    "    \n",
    "    for run in runs_data:\n",
    "        outer_idx = outer_lrs.index(run[\"config\"].get(\"outer_learning_rate\"))\n",
    "        inner_idx = inner_lrs.index(run[\"config\"].get(\"inner_learning_rate\"))\n",
    "        key = (inner_idx, outer_idx)\n",
    "        total_counts[key] += 1\n",
    "        if run[\"is_success\"]:\n",
    "            success_counts[key] += 1\n",
    "    \n",
    "    for (inner_idx, outer_idx), total in total_counts.items():\n",
    "        successes = success_counts.get((inner_idx, outer_idx), 0)\n",
    "        grid[inner_idx, outer_idx] = successes / total if total > 0 else 0\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    im = plt.imshow(grid, origin='lower', cmap='viridis', interpolation='nearest', vmin=0, vmax=1)\n",
    "    plt.colorbar(im, label='Fraction of Successful Runs')\n",
    "    \n",
    "    for i in range(len(inner_lrs)):\n",
    "        for j in range(len(outer_lrs)):\n",
    "            fraction = grid[i, j]\n",
    "            if total_counts.get((i, j), 0) > 0:\n",
    "                successes = success_counts.get((i, j), 0)\n",
    "                total = total_counts.get((i, j), 0)\n",
    "                text = f\"{successes}/{total}\"\n",
    "            else:\n",
    "                text = \"N/A\"\n",
    "            plt.text(j, i, text, ha='center', va='center', color='white' if fraction < 0.5 else 'black')\n",
    "    \n",
    "    plt.xticks(np.arange(len(outer_lrs)), [f\"{lr:.1e}\" for lr in outer_lrs], rotation=45)\n",
    "    plt.yticks(np.arange(len(inner_lrs)), [f\"{lr:.1e}\" for lr in inner_lrs])\n",
    "    plt.xlabel('Outer Learning Rate')\n",
    "    plt.ylabel('Inner Learning Rate')\n",
    "    title = f'Success Fraction Heatmap for ts={ts} in {group_name}'\n",
    "    if subfolder:\n",
    "        title += f' ({subfolder})'\n",
    "    if inner_optimizer:\n",
    "        title += f' (inner_optimizer={inner_optimizer})'\n",
    "    plt.title(title)\n",
    "    \n",
    "    group_results_dir = os.path.join(results_dir, group_name, subfolder or '')\n",
    "    os.makedirs(group_results_dir, exist_ok=True)\n",
    "    output_file = os.path.join(group_results_dir, f'heatmap_ts_group_{group_idx}.png')\n",
    "    plt.savefig(output_file, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    logger.info(f\"Saved heatmap for ts={ts} in {group_name}{f'/{subfolder}' if subfolder else ''}{f' (inner_optimizer={inner_optimizer})' if inner_optimizer else ''} to {output_file}\")\n",
    "\n",
    "# Function to create aggregated heatmap\n",
    "def create_aggregated_heatmap(all_runs_data, group_name, subfolder=None, inner_optimizer=None):\n",
    "    if inner_optimizer:\n",
    "        all_runs_data = [run for run in all_runs_data if inner_optimizer in (run[\"config\"].get(\"inner_optimizer\", [\"unknown\"]) if isinstance(run[\"config\"].get(\"inner_optimizer\"), list) else [run[\"config\"].get(\"inner_optimizer\", \"unknown\")])]\n",
    "    outer_lrs = sorted(set(run[\"config\"].get(\"outer_learning_rate\") for run in all_runs_data))\n",
    "    inner_lrs = sorted(set(run[\"config\"].get(\"inner_learning_rate\") for run in all_runs_data))\n",
    "    \n",
    "    if not outer_lrs or not inner_lrs:\n",
    "        logger.warning(f\"No valid learning rates for aggregated heatmap in {group_name}{f'/{subfolder}' if subfolder else ''}{f' (inner_optimizer={inner_optimizer})' if inner_optimizer else ''}\")\n",
    "        return\n",
    "    \n",
    "    grid = np.zeros((len(inner_lrs), len(outer_lrs)))\n",
    "    success_counts = defaultdict(int)\n",
    "    total_counts = defaultdict(int)\n",
    "    \n",
    "    for run in all_runs_data:\n",
    "        outer_idx = outer_lrs.index(run[\"config\"].get(\"outer_learning_rate\"))\n",
    "        inner_idx = inner_lrs.index(run[\"config\"].get(\"inner_learning_rate\"))\n",
    "        key = (inner_idx, outer_idx)\n",
    "        total_counts[key] += 1\n",
    "        if run[\"is_success\"]:\n",
    "            success_counts[key] += 1\n",
    "    \n",
    "    for (inner_idx, outer_idx), total in total_counts.items():\n",
    "        successes = success_counts.get((inner_idx, outer_idx), 0)\n",
    "        grid[inner_idx, outer_idx] = successes / total if total > 0 else 0\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    im = plt.imshow(grid, origin='lower', cmap='viridis', interpolation='nearest', vmin=0, vmax=1)\n",
    "    plt.colorbar(im, label='Fraction of Successful Runs')\n",
    "    \n",
    "    for i in range(len(inner_lrs)):\n",
    "        for j in range(len(outer_lrs)):\n",
    "            fraction = grid[i, j]\n",
    "            if total_counts.get((i, j), 0) > 0:\n",
    "                successes = success_counts.get((i, j), 0)\n",
    "                total = total_counts.get((i, j), 0)\n",
    "                text = f\"{successes}/{total}\"\n",
    "            else:\n",
    "                text = \"N/A\"\n",
    "            plt.text(j, i, text, ha='center', va='center', color='white' if fraction < 0.5 else 'black')\n",
    "    \n",
    "    plt.xticks(np.arange(len(outer_lrs)), [f\"{lr:.1e}\" for lr in outer_lrs], rotation=45)\n",
    "    plt.yticks(np.arange(len(inner_lrs)), [f\"{lr:.1e}\" for lr in inner_lrs])\n",
    "    plt.xlabel('Outer Learning Rate')\n",
    "    plt.ylabel('Inner Learning Rate')\n",
    "    title = f'Aggregated Success Fraction Heatmap in {group_name}'\n",
    "    if subfolder:\n",
    "        title += f' ({subfolder})'\n",
    "    if inner_optimizer:\n",
    "        title += f' (inner_optimizer={inner_optimizer})'\n",
    "    plt.title(title)\n",
    "    \n",
    "    group_results_dir = os.path.join(results_dir, group_name, subfolder or '')\n",
    "    os.makedirs(group_results_dir, exist_ok=True)\n",
    "    output_file = os.path.join(group_results_dir, 'heatmap_aggregated.png')\n",
    "    plt.savefig(output_file, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    logger.info(f\"Saved aggregated heatmap in {group_name}{f'/{subfolder}' if subfolder else ''}{f' (inner_optimizer={inner_optimizer})' if inner_optimizer else ''} to {output_file}\")\n",
    "\n",
    "# Function to create ts grid heatmap\n",
    "def create_ts_grid_heatmap(ts_groups, group_name, optimizer):\n",
    "    subfolder = f\"inner_optimizer_{sanitize_folder_name(optimizer)}\"\n",
    "    group_results_dir = os.path.join(results_dir, group_name, subfolder)\n",
    "    os.makedirs(group_results_dir, exist_ok=True)\n",
    "    \n",
    "    fig, axes = plt.subplots(4, 5, figsize=(25, 20))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    ts_list = sorted(ts_groups.keys())[:20]\n",
    "    \n",
    "    for idx, ax in enumerate(axes):\n",
    "        if idx < len(ts_list):\n",
    "            ts = ts_list[idx]\n",
    "            runs_data = [run for run in ts_groups[ts] if optimizer in (run[\"config\"].get(\"inner_optimizer\", [\"unknown\"]) if isinstance(run[\"config\"].get(\"inner_optimizer\"), list) else [run[\"config\"].get(\"inner_optimizer\", \"unknown\")])]\n",
    "            outer_lrs = sorted(set(run[\"config\"].get(\"outer_learning_rate\") for run in runs_data))\n",
    "            inner_lrs = sorted(set(run[\"config\"].get(\"inner_learning_rate\") for run in runs_data))\n",
    "            \n",
    "            if not outer_lrs or not inner_lrs:\n",
    "                logger.warning(f\"No valid learning rates for ts group {ts} in {group_name}/{subfolder}\")\n",
    "                ax.axis('off')\n",
    "                continue\n",
    "            \n",
    "            grid = np.zeros((len(inner_lrs), len(outer_lrs)))\n",
    "            success_counts = defaultdict(int)\n",
    "            total_counts = defaultdict(int)\n",
    "            \n",
    "            for run in runs_data:\n",
    "                outer_idx = outer_lrs.index(run[\"config\"].get(\"outer_learning_rate\"))\n",
    "                inner_idx = inner_lrs.index(run[\"config\"].get(\"inner_learning_rate\"))\n",
    "                key = (inner_idx, outer_idx)\n",
    "                total_counts[key] += 1\n",
    "                if run[\"is_success\"]:\n",
    "                    success_counts[key] += 1\n",
    "            \n",
    "            for (inner_idx, outer_idx), total in total_counts.items():\n",
    "                successes = success_counts.get((inner_idx, outer_idx), 0)\n",
    "                grid[inner_idx, outer_idx] = successes / total if total > 0 else 0\n",
    "            \n",
    "            im = ax.imshow(grid, origin='lower', cmap='viridis', interpolation='nearest', vmin=0, vmax=1)\n",
    "            \n",
    "            for i in range(len(inner_lrs)):\n",
    "                for j in range(len(outer_lrs)):\n",
    "                    fraction = grid[i, j]\n",
    "                    if total_counts.get((i, j), 0) > 0:\n",
    "                        successes = success_counts.get((i, j), 0)\n",
    "                        total = total_counts.get((i, j), 0)\n",
    "                        text = f\"{successes}/{total}\"\n",
    "                    else:\n",
    "                        text = \"N/A\"\n",
    "                    ax.text(j, i, text, ha='center', va='center', color='white' if fraction < 0.5 else 'black', fontsize=10)\n",
    "            \n",
    "            ax.set_xticks(np.arange(len(outer_lrs)))\n",
    "            ax.set_yticks(np.arange(len(inner_lrs)))\n",
    "            ax.set_xticklabels([f\"{lr:.1e}\" for lr in outer_lrs], rotation=45, fontsize=8)\n",
    "            ax.set_yticklabels([f\"{lr:.1e}\" for lr in inner_lrs], fontsize=8)\n",
    "            \n",
    "            ax.set_xlabel('Outer LR', fontsize=10)\n",
    "            ax.set_ylabel('Inner LR', fontsize=10)\n",
    "            ax.set_title(f\"ts={ts}\", fontsize=12)\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Success Fraction Heatmaps for inner_optimizer={optimizer}', fontsize=20)\n",
    "    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "    plt.colorbar(im, cax=cbar_ax, label='Fraction of Successful Runs')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 0.9, 0.95])\n",
    "    output_file = os.path.join(group_results_dir, 'grid_4x5_ts_heatmaps.png')\n",
    "    plt.savefig(output_file, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    logger.info(f\"Saved 4x5 grid heatmap for inner_optimizer={optimizer} in {group_name}/{subfolder} to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_runs_data = []\n",
    "for result in process_results:\n",
    "    if result[\"status\"] == \"success\" and result[\"data\"]:\n",
    "        all_runs_data.append(result[\"data\"])\n",
    "\n",
    "# Organize runs by ts groups\n",
    "ts_groups = defaultdict(list)\n",
    "for run in all_runs_data:\n",
    "    ts = tuple(run[\"config\"].get(\"ts\", ()))\n",
    "    ts_groups[ts].append(run)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate heatmaps for each ts group\n",
    "for idx, (ts, runs_data) in enumerate(ts_groups.items()):\n",
    "    logger.info(f\"Generating heatmap for ts group {ts} in {group_name}\")\n",
    "    create_heatmap(ts, runs_data, idx, group_name)\n",
    "\n",
    "# Generate aggregated heatmap\n",
    "logger.info(f\"Generating aggregated heatmap in {group_name}\")\n",
    "create_aggregated_heatmap(all_runs_data, group_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate heatmaps for each optimizer\n",
    "optimizer_groups = set()\n",
    "for run_data in all_runs_data:\n",
    "    inner_optimizer = run_data[\"config\"].get(\"inner_optimizer\", \"unknown\")\n",
    "    inner_optimizer = inner_optimizer if isinstance(inner_optimizer, list) else [inner_optimizer]\n",
    "    for optimizer in inner_optimizer:\n",
    "        optimizer_groups.add(optimizer)\n",
    "\n",
    "for optimizer in optimizer_groups:\n",
    "    subfolder = f\"inner_optimizer_{sanitize_folder_name(optimizer)}\"\n",
    "    logger.info(f\"Generating heatmaps for inner_optimizer={optimizer} in {group_name}/{subfolder}\")\n",
    "    for idx, (ts, runs_data) in enumerate(ts_groups.items()):\n",
    "        logger.info(f\"Generating heatmap for ts group {ts} in {group_name}/{subfolder}\")\n",
    "        create_heatmap(ts, runs_data, idx, group_name, subfolder, inner_optimizer=optimizer)\n",
    "    logger.info(f\"Generating aggregated heatmap in {group_name}/{subfolder}\")\n",
    "    create_aggregated_heatmap(all_runs_data, group_name, subfolder, inner_optimizer=optimizer)\n",
    "    \n",
    "    # Generate 4x5 grid heatmap for SGD optimizer\n",
    "    if optimizer == \"sgd\":\n",
    "        logger.info(f\"Generating 4x5 grid heatmap for inner_optimizer={optimizer} in {group_name}/{subfolder}\")\n",
    "        create_ts_grid_heatmap(ts_groups, group_name, optimizer)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
